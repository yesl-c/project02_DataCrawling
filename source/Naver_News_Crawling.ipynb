{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from random import *\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from datetime import timedelta   # 날짜 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20200602'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test\n",
    "d = datetime.datetime(2020,6,2)               # 2020년 6월2일\n",
    "YMD = d.strftime(\"%Y-%m-%d\").replace('-','')  # 2020-06-02에서 -제외하고 문자열로 반환\n",
    "YMD                                           # 문자열 20200602"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 신문기사 헤드라인 크롤링\n",
    "* 수집할 웹 페이지 : 네이버 뉴스 정치 속보 전체\n",
    "* 수집기간 : 2019.09.01 ~ 2020.06.02\n",
    "* 수집내용 : 기사 헤드라인 (동영상기사 제외)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome('chromedriver83.exe')\n",
    "driver.implicitly_wait(10)\n",
    "url = 'https://news.naver.com/main/list.nhn?mode=LSD&sid1=100&mid=sec&listType=paper&date=20200602&page=1'\n",
    "driver.get(url)\n",
    "\n",
    "article_count = 0      # 한페이지 내 기사수\n",
    "page = 1               # 페이지 넘기기위한 변수\n",
    "press_count = []       # 언론사 이름저장\n",
    "article_contents = []  # 기사제목 저장\n",
    "\n",
    "while True:\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    article = soup.find_all('a', class_='nclicks(cnt_papaerart1)') # 한 페이지내에 기사제목 전체를 긁어옴\n",
    "    press = soup.find_all('span', class_='writing')                # 언론사이름\n",
    "    \n",
    "    for j in press:\n",
    "        press_count.append(j.text) # 언론사 이름 press_count에 저장\n",
    "    \n",
    "    for i in article: # 기사제목을 하나씩 읽어들임\n",
    "        article_count += 1               # 기사제목 하나 읽을때마다 변수값이 1씩 증가\n",
    "        article_contents.append(i.text)  # article_contents에 기사제목 저장\n",
    "        \n",
    "    if article_count == 20: # 한페이지를 다읽었으면\n",
    "        article_count = 0   # 0으로 초기화\n",
    "        page +=1            # 다음 페이지로 넘어감\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&sid1=100&mid=sec&listType=paper&date='+YMD+'&page='+str(page)\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        article2 = soup.find_all('a', class_='nclicks(cnt_papaerart1)') # 다음페이지 정보를 긁어와서\n",
    "        if article == article2: # 이전페이지와 현재페이지 내용이 같으면 (즉, 그날의 마지막 페이지이면)\n",
    "            page = 0\n",
    "            d = d - timedelta(days=1) # 어제 날짜로 변경\n",
    "            YMD = d.strftime(\"%Y-%m-%d\").replace('-','')\n",
    "        if YMD == '20190901': # 2019년 9월 1일이 되면\n",
    "            break             # 데이터 크롤링 종료\n",
    "            \n",
    "        time.sleep(2)\n",
    "    else: # 페이지내에서 읽어온 기사제목이 20개가 안되는 경우 (즉, 마지막 페이지이면 아까와 동일하게)\n",
    "        article_count = 0\n",
    "        page +=1\n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&sid1=100&mid=sec&listType=paper&date='+YMD+'&page='+str(page)\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        soup = BeautifulSoup(driver.page_source,'html.parser')\n",
    "        \n",
    "        article2 = soup.find_all('a', class_='nclicks(cnt_papaerart1)')\n",
    "        if article == article2: # 마지막 페이지가 맞는지 확실히 검사\n",
    "            page = 0\n",
    "            d = d - timedelta(days=1)\n",
    "            YMD = d.strftime(\"%Y-%m-%d\").replace('-','')\n",
    "            \n",
    "            if YMD == '20190901':\n",
    "                break\n",
    "                \n",
    "            time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "동영상기사 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_values_from_list(the_list, val): \n",
    "    return [value for value in the_list if value != val]\n",
    "\n",
    "article_contents = remove_values_from_list(article_contents,'동영상기사')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수집된 데이터 저장(통합)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = {'article':article_contents, 'press':press_count}\n",
    "df = pd.DataFrame(dat)\n",
    "df.to_csv('naver.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "수집된 데이터 저장(언론사 별로)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = ['경향신문','국민일보','동아일보','디지털타임스','매일경제','머니투데이','문화일보',\n",
    "        '서울경제','서울신문','세계일보','아시아경제','이데일리','전자신문','조선일보',\n",
    "        '중앙일보','파이낸셜뉴스','한겨레','한국경제','한국일보','헤럴드경제']\n",
    "\n",
    "for i in range dict:    # 언론사 별로 csv파일 저장\n",
    "    h = df.loc[df['press'] == i]\n",
    "    h.to_csv(i +'.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 검증\n",
    "* 실행시 언론사이름 입력하면 전체 크롤링 자료와 언론사별 분류자료가 일치하는지 확인\n",
    "* 올바르지 않은 언론사이름 입력시 다시 입력받음\n",
    "* 끝 입력시 반복문 종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    word = input(\"언론사 이름 입력 : \")\n",
    "    \n",
    "    if word in dict:\n",
    "        b = pd.read_csv(word+'.csv',encoding='utf-8')\n",
    "        h = df.loc[df['press'] == word]\n",
    "        h2 = b.loc[:]\n",
    "        \n",
    "        if len(h) == len(h2):\n",
    "            print(len(h) ,'개의 기사가 존재')\n",
    "            print(\"검증 완료!\")\n",
    "        elif len(h) != len(h2):\n",
    "            print(\"기사 길이가 다릅니다. 검증 실패\")\n",
    "            \n",
    "    elif word == \"끝\":\n",
    "        print(\"종료!\")\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        print(\"정확한 언론사를 입력해주세요!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
